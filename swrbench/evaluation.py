import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os
import re
import threading
import numpy as np
import logging
import random
from tqdm import tqdm
from utils import run_chat

DEFECT_TYPE_TEXT_MAP = {
    'E.1': 'Documentation',
    'E.1.1': 'Textual Changes',
    'E.1.2': 'Language Features',
    'E.2': 'Visual Representation',
    'E.3': 'Structure',
    'E.3.1': 'Organization',
    'E.3.2': 'Solution Approach',
    'F.1': 'Interface',
    'F.2': 'Logic',
    'F.3': 'Resource',
    'F.4': 'Check',
    'F.5': 'Support',
    'F.6': 'Larger Defects',
}

def load_jsonl(path):
    with open(path, "r") as f:
        data = [json.loads(line) for line in f.readlines()]
    return data


def save_jsonl(data, path):
    with open(path, "w") as f:
        for item in data:
            f.write(json.dumps(item) + "\n")


def save_json(data, path):
    with open(path, "w") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def load_json(path):
    with open(path, "r") as f:
        data = json.load(f)
    return data

def create_clean_pr_prompt(item):
    instance = item['instance']
    pred = item['pred']
    pr_title = instance["pr_title"]
    pr_statement = instance["pr_statement"]
    # For clean PRs, the ground truth is simply that it's clean.
    # We don't need the full timeline here, just the confirmation.
    # pr_timeline = instance["pr_timeline"] 
    pred_review = pred["review"]
    
    # Explicitly state the ground truth for a clean PR
    ground_truth_status = "The pull request is confirmed to be good and clean; no actual defects were introduced."
    
    return (
        "You are an objective Evaluation Assistant. Your task is to evaluate a predicted code review for a **clean** pull request (one that introduces no actual defects).\n"
        "Your primary goals are to determine if the predicted review correctly identifies the pull request as clean and, if it raises any issues, to assess the **severity of those potential false positives**.\n"
        "**CONTEXT:**\n"
        "<Pull Request Details>\n"
        f"- **Pull Request Title**: {pr_title}\n"
        f"- **Pull Request Description**: {pr_statement}\n"
        "</Pull Request Details>\n"
        "<Ground Truth Status>\n"
        f"{ground_truth_status}\n"
        "</Ground Truth Status>\n"
        "<Predicted Review>\n"
        "This is the review result generated by the system being evaluated.\n"
        f"{pred_review.strip()}\n"
        "</Predicted Review>\n"
        "**EVALUATION TASK:**\n"
        "Follow these steps precisely:\n"
        "1.  **Analyze Predicted Review Content:**\n"
        "    *   Carefully read the `<Predicted Review>`. \n"
        "    *   Determine if the review **explicitly states or strongly implies** the PR is clean, good, approved, or ready to merge (e.g., using phrases like 'LGTM', 'Approved', 'Looks good', 'No issues found'). Record this as YES or NO for the 'Correctly Identified as Good' output field. Assess this based *only* on such explicit statements, regardless of whether minor suggestions are also present.\n"
        "    *   Identify all potential issues, defects, or suggestions raised in the review text.\n"
        "2.  **Identify and Assess Severity of Predicted Issues (False Positives):**\n"
        "    *   If the `<Predicted Review>` raises *any* specific issues, comments, or suggestions (potential false positives), identify each distinct point. \n"
        "    *   Assign a unique sequential ID to each identified point, starting from `PRED-ISSUE-1`, `PRED-ISSUE-2`, etc. \n"
        "    *   Extract the core description of the point from the `<Predicted Review>`. \n"
        "    *   For *each* identified point (`PRED-ISSUE-1`, `PRED-ISSUE-2`, etc.), evaluate its **inherent severity** based on the *nature* of the issue raised. Use the following 0-10 scale, where higher scores indicate more severe potential issues: \n"
        "        *   0: Trivial (e.g., optional style comment, minor non-blocking doc clarification).\n"
        "        *   1-3: Low (e.g., typo, code formatting, slightly confusing naming, minor documentation inaccuracy).\n"
        "        *   4-6: Moderate (e.g., potential minor logic flaw, unclear API usage, possible inefficiency, deviation from best practices).\n"
        "        *   7-9: High (e.g., likely significant logic error, potential crash/exception, resource leak, security concern).\n"
        "        *   10: Critical (e.g., clear security vulnerability, data corruption risk, major functionality failure).\n"
        "**OUTPUT FORMAT:**\n"
        "Present your evaluation strictly in the following format. Do not add any introductory or concluding remarks outside this structure.\n"
        "**EVALUATION RESULT (Clean PR):**\n"
        "**1. Clean PR Identification:**\n"
        "Correctly Identified as Good: [YES or NO - Based on Step 1 analysis]\n\n"
        "**2. Analysis of Predicted Review Issues (Potential False Positives):**\n"
        "[If no issues were raised in the Predicted Review, state: 'No specific issues were raised in the predicted review.']\n"
        "[Otherwise, list each identified issue:]\n"
        "<Predicted Issue PRED-ISSUE-1>\n"
        "Description: [Extract the description of the first point identified from the <Predicted Review>]\n"
        "Severity Score: [Assign the score (0-10) based on the criteria in Step 2]\n"
        "</Predicted Issue PRED-ISSUE-1>\n"
        "<Predicted Issue PRED-ISSUE-2>\n"
        "Description: [Extract the description of the second point identified from the <Predicted Review>]\n"
        "Severity Score: [Assign the score (0-10) based on the criteria in Step 2]\n"
        "</Predicted Issue PRED-ISSUE-2>\n"
        "[... Continue for all identified predicted issues ...]\n"
        "**END OF EVALUATION RESULT**\n"
    )
    

def parse_clean_pr_answer(args, instance, answer, logger):
    prompt = (
        "You are a data extraction assistant. Your task is to meticulously parse the provided text, which contains an evaluation result for a **clean** pull request, and transform it into a structured JSON object.\n\n"
        "**Input Text Format:**\n"
        "The input text starts with a section indicating if the PR was identified as clean, followed by blocks describing predicted issues (potential false positives), following this pattern:\n"
        "**1. Clean PR Identification:**\n"
        "Correctly Identified as Good: [YES or NO]\n\n"
        "**2. Analysis of Predicted Review Issues (Potential False Positives):**\n"
        "[Optional: Text indicating no issues were raised OR <Predicted Issue ...> blocks]\n"
        "<Predicted Issue PRED-ISSUE-ID>\n"
        "Description: [Text describing the predicted issue]\n"
        "Severity Score: [A score from 0-10]\n"
        "</Predicted Issue PRED-ISSUE-ID>\n\n"
        "There can be multiple <Predicted Issue ...> blocks. If no issues were raised, the text might indicate that explicitly in section 2.\n\n"
        "**Parsing Instructions:**\n"
        "1.  Extract the value ('YES' or 'NO') following 'Correctly Identified as Good: '.\n"
        "2.  Check section 2. If it indicates that 'No specific issues were raised', the `\"pred_issues\"` dictionary in the output JSON should be empty.\n"
        "3.  If issues are present in section 2, identify *all* blocks starting with `<Predicted Issue ...>`.\n"
        "4.  For each predicted issue block:\n"
        "    - Extract the ID (e.g., 'PRED-ISSUE-1') from the tag.\n"
        "    - Extract the text following 'Description: '.\n"
        "    - Extract the number following 'Severity Score: ' and convert it to an **integer**.\n"
        "5.  Construct a single JSON object based on the extracted information.\n\n"
        "**Target JSON Structure:**\n"
        "The final output must be a JSON object with exactly two top-level keys:\n"
        "- `\"correctly_identified_as_good\"`: The extracted status string ('YES' or 'NO'), based on explicit statements in the review.\n"
        "- `\"pred_issues\"`: A dictionary where:\n"
        "    - Keys are the extracted predicted issue IDs (e.g., \"PRED-ISSUE-1\").\n"
        "    - Values are dictionaries containing:\n"
        "        - `\"description\"`: The extracted description string.\n"
        "        - `\"severity_score\"`: The extracted severity score as an **integer**.\n"
        "- If no specific issues were raised in the input text, this should be an empty dictionary `{}`.\n\n"
        "**Example JSON Output (Explicitly Identified Clean, with negligible/minor suggestions):**\n"
        "```json\n"
        '{\n'
        '  "correctly_identified_as_good": "YES",\n'
        '  "pred_issues": {\n'
        '    "PRED-ISSUE-1": {"description": "Minor documentation suggestion.", "severity_score": 0}\n'
        '  }\n'
        '}\n'
        "```\n\n"
        "**Example JSON Output (Not Explicitly Identified Clean, With Issues):**\n"
        "```json\n"
        '{\n'
        '  "correctly_identified_as_good": "NO",\n'
        '  "pred_issues": {\n'
        '     "PRED-ISSUE-1": {"description": "Claims a non-existent logic error.", "severity_score": 5},\n'
        '     "PRED-ISSUE-2": {"description": "Pedantic style issue presented as blocking.", "severity_score": 2}\n'
        '   }\n'
        '}\n'
        "```\n\n"
        "**Example JSON Output (Not Explicitly Identified Clean, No Specific Issues Mentioned):**\n"
        "```json\n"
        '{\n'
        '  "correctly_identified_as_good": "NO",\n'
        '  "pred_issues": {}\n'
        '}\n'
        "```\n\n"
        "**Output Requirements:**\n"
        "- Return **ONLY** the JSON object.\n"
        "- Enclose the JSON object in a markdown code block starting with ```json.\n"
        "- Do **not** include any explanations, apologies, or introductory/concluding text outside the JSON code block.\n\n"
        f"**Text to Parse:**\n\n"
        f"```markdown\n{answer}\n```"
    )

    parsed_answer = run_chat(
        model=args.model, 
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0, 
        max_tokens=8192 # Adjusted max_tokens based on potential complexity
    )
    if parsed_answer is None:
        logger.error("Failed to get parsing response from model.")
        return None
    
    # Extract JSON using regex - use a more robust approach to find content between ```json and ``` markers
    json_str = None
    if '```json' in parsed_answer:
        start_marker = '```json'
        end_marker = '```'
        start_pos = parsed_answer.find(start_marker) + len(start_marker)
        
        # Find the last occurence of ``` that comes after the ```json marker
        remaining_text = parsed_answer[start_pos:]
        end_relative_pos = remaining_text.rfind(end_marker)
        if end_relative_pos != -1:
            end_pos = start_pos + end_relative_pos
            json_str = parsed_answer[start_pos:end_pos].strip()
    
    if not json_str:
        # Handle cases where the model might just return {} directly without markdown
        # Need to adjust this for the new structure
        # Example: {"correctly_identified_as_good": "YES", "pred_issues": {}}
        stripped_answer = parsed_answer.strip()
        if stripped_answer.startswith('{') and stripped_answer.endswith('}'):
            try:
                potential_json = json.loads(stripped_answer)
                if "correctly_identified_as_good" in potential_json and "pred_issues" in potential_json:
                    json_str = stripped_answer # Assume it's valid JSON
            except json.JSONDecodeError:
                pass # Ignore if it's not valid JSON

        if not json_str:
            logger.error(f"No JSON block found or direct JSON parse failed in parsing response: {parsed_answer}")
            return None
        
    try:
        json_str = re.sub(r'[\x00-\x1F\x7F]', '', json_str)
        result = json.loads(json_str)
            
        # --- Start Validation for Clean Task Output --- 
        if "correctly_identified_as_good" not in result:
            logger.error(f"Missing 'correctly_identified_as_good' field: {json_str}")
            return None
            
        if result["correctly_identified_as_good"] not in ["YES", "NO"]:
            logger.error(f"Invalid value for 'correctly_identified_as_good': '{result['correctly_identified_as_good']}'. Must be 'YES' or 'NO'. JSON: {json_str}")
            return None

        if "pred_issues" not in result:
            logger.error(f"Missing 'pred_issues' field: {json_str}")
            return None
        
        if not isinstance(result['pred_issues'], dict):
             logger.error(f"'pred_issues' field is not a dictionary: {json_str}")
             return None
            
        # Validate predicted issues structure if any exist
        for issue_id, issue_info in result['pred_issues'].items():
            if not issue_id.startswith("PRED-ISSUE-"):
                 logger.warning(f"Unexpected format for issue ID '{issue_id}'. Expected 'PRED-ISSUE-X'.")
                 # Allow processing but log a warning
            if "description" not in issue_info:
                logger.error(f"Missing 'description' field for issue {issue_id}: {json_str}")
                return None
            if "severity_score" not in issue_info:
                logger.error(f"Missing 'severity_score' field for issue {issue_id}: {json_str}")
                return None
            
            # Ensure severity scores are integers
            try:
                issue_info['severity_score'] = int(issue_info['severity_score'])
            except ValueError:
                 logger.error(f"'severity_score' is not an integer for issue {issue_id}: {json_str}")
                 return None
            except TypeError:
                 logger.error(f"'severity_score' is not a number for issue {issue_id}: {json_str}")
                 return None
        # --- End Validation for Clean Task Output --- 
        
        # Return the structure directly as parsed
        return result
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON decode error: {e} in: {json_str}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing clean task answer: {e}")
        return None


def create_defect_pr_prompt(item):
    instance = item['instance']
    pred = item['pred']
    pr_title = instance["pr_title"]
    pr_statement = instance["pr_statement"]
    pr_timeline = instance["pr_timeline"]
    pred_review = pred["review"]
    
    defects_description = []
    for i, defect in enumerate(instance['defects']):
        defects_description.append(
            f"<Ground Truth Defect GT-ISSUE-{i+1}>\n"
            f"Defect Type: {defect['defect_type']} {DEFECT_TYPE_TEXT_MAP[defect['defect_type']]}\n"
            f"Defect Description: {defect['defect_discussion']['description']}\n"
            f"Defect Code Snippet: {defect['defect_introducing_commit']['code']}\n"
            f"</Ground Truth Defect GT-ISSUE-{i+1}>\n"
        )   
    
    # Extract ground truth review content from timeline
    ground_truth_reviews = []
    for item in pr_timeline:
        if item['type'] == 'description':
            continue  
        elif item['type'] == 'comment':
            ground_truth_reviews.append(f"<Start of Comment>\nTime: {item['created_at']}\nAuthor: {item['user']}\nComment: {item['body']}\n<End of Comment>")
        elif item['type'] == 'review_comment':
            reply_comments = ""
            for comment in item['reply']:
                reply_comments += f"<Start of Sub Review Comment>\nTime: {comment['created_at']}\nAuthor: {comment['user']}\nComment: {comment['body']}\n<End of Sub Review Comment>\n"
            ground_truth_reviews.append(f"<Start of Review Comment>\n<Start of Related Diff Hunk>\nFile: {item['path']}\n{item['diff_hunk']}\n<End of Related Diff Hunk>\n{reply_comments}<End of Review Comment>")
        elif item['type'] == 'commit':            
            ground_truth_reviews.append(f"<Start of Commit>\nTime: {item['date']}\nSHA: {item['sha']}\nAuthor: {item['author']}\nMessage: {item['message']}\n<End of Commit>")
        elif item['type'] == 'review':
            ground_truth_reviews.append(f"<Start of Review>\nTime: {item['created_at']}\nAuthor: {item['user']}\nReview: {item['body']}\n<End of Review>")
    
    assert len(ground_truth_reviews) > 0, "No ground truth review content available."
    
    ground_truth_reviews = "\n".join(ground_truth_reviews)
    defects_description = "\n".join(defects_description)
    
    return (
        "You are an objective Evaluation Assistant. Your task is to evaluate a predicted code review based on how accurately and usefully it identifies the actual defects present in a pull request, comparing it against a ground truth list of defects.\n"
        "**CONTEXT:**\n"
        "<Pull Request Details>\n"
        f"- **Pull Request Title**: {pr_title}\n"
        f"- **Pull Request Description**: {pr_statement}\n"
        f"- **Pull Request Review Content**: \n{ground_truth_reviews}\n"
        "</Pull Request Details>\n"
        "<Ground Truth Defects>\n"
        "This section lists the actual defects confirmed to be introduced by the pull request. Each defect has an ID (e.g., GT-ISSUE-1), a type, and a description.\n"
        f"{defects_description.strip()}\n"
        "</Ground Truth Defects>\n"
        "<Predicted Review>\n"
        "This is the review result generated by the system being evaluated.\n"
        f"{pred_review.strip()}\n"
        "</Predicted Review>\n"
        "**EVALUATION TASK:**\n"
        "Follow these steps precisely:\n"
        "1.  **Assess Overall Stance:**\n"
        "    *   Read the `<Predicted Review>`. Determine if it **incorrectly** claims or strongly implies the PR is clean, good, approved, or ready to merge (e.g., saying 'LGTM', 'Approved', 'No issues') despite the presence of `<Ground Truth Defects>`. \n"
        "    *   Record this assessment as YES or NO for the 'Incorrectly Identified as Good' output field.\n"
        "2.  **Identify Predicted Defects:**\n"
        "    *   Carefully read the `<Predicted Review>` again. \n"
        "    *   Identify each distinct potential defect or issue mentioned (even if the overall stance was incorrectly positive). \n"
        "    *   Assign a unique sequential ID to each identified issue, starting from `PRED-ISSUE-1`, `PRED-ISSUE-2`, etc. \n"
        "    *   Extract the core description of the issue from the `<Predicted Review>`. \n"
        "3.  **Assess Severity of Predicted Defects:**\n"
        "    *   For *each* defect identified in Step 2 (`PRED-ISSUE-1`, `PRED-ISSUE-2`, etc.), evaluate its **inherent severity** based on the *nature* of the issue raised (independent of whether it matches a ground truth defect). \n"
        "    *   Use the following 0-10 scale, where higher scores indicate more severe potential issues: \n"
        "        *   0: Trivial (e.g., optional style comment, minor non-blocking doc clarification).\n"
        "        *   1-3: Low (e.g., typo, code formatting, slightly confusing naming, minor documentation inaccuracy).\n"
        "        *   4-6: Moderate (e.g., potential minor logic flaw, unclear API usage, possible inefficiency, deviation from best practices).\n"
        "        *   7-9: High (e.g., likely significant logic error, potential crash/exception, resource leak, security concern).\n"
        "        *   10: Critical (e.g., clear security vulnerability, data corruption risk, major functionality failure).\n"
        "4.  **Match Ground Truth Defects to Predicted Defects:**\n"
        "    *   For *each* defect listed in `<Ground Truth Defects>` (e.g., `GT-ISSUE-1`, `GT-ISSUE-2`, etc.): \n"
        "        *   Determine if any of the predicted defects identified in Step 2 (`PRED-ISSUE-1`, `PRED-ISSUE-2`, etc.) successfully hit this ground truth defect based on their **semantic meaning and description**, irrespective of the Severity Score assigned in Step 3. \n"
        "        *   A `Hit` occurs if a predicted defect *substantially and correctly* identifies the ground truth defect. \n"
        "        *   If multiple predicted defects seem relevant, choose the one that provides the *most accurate and complete semantic* description of the ground truth defect as the primary hit. \n"
        "        *   Record `Hit: YES` or `Hit: NO`. \n"
        "        *   If `Hit: YES`, record the ID (e.g., `PRED-ISSUE-1`) of the predicted defect that hit it in the `Hit by:` field. If `Hit: NO`, leave `Hit by: N/A`. \n"
        "**OUTPUT FORMAT:**\n"
        "Present your evaluation strictly in the following format. Do not add any introductory or concluding remarks outside this structure.\n"
        "**EVALUATION RESULT:**\n"
        "**1. Clean/Good Identification Assessment:**\n"
        "Incorrectly Identified as Clean/Good: [YES or NO - Based on Step 1 analysis]\n\n"
        "**2. Analysis of Predicted Review Defects:**\n"
        "<Predicted Defect PRED-ISSUE-1>\n"
        "Description: [Extract the description of the first defect identified from the <Predicted Review>]\n"
        "Severity Score: [Assign the inherent severity score (0-10) based on the criteria in Step 3]\n"
        "</Predicted Defect PRED-ISSUE-1>\n"
        "<Predicted Defect PRED-ISSUE-2>\n"
        "Description: [Extract the description of the second defect identified from the <Predicted Review>]\n"
        "Severity Score: [Assign the inherent severity score (0-10) based on the criteria in Step 3]\n"
        "</Predicted Defect PRED-ISSUE-2>\n"
        "[... Continue for all identified predicted defects ...]\n"
        "**3. Ground Truth Defect Coverage:**\n"
        "<Ground Truth Defect GT-ISSUE-1>\n"
        "Defect Type: [Copy from the corresponding Ground Truth Defect input]\n"
        "Description: [Copy from the corresponding Ground Truth Defect input]\n"
        "Hit: [YES or NO]\n"
        "Hit by: [PRED-ISSUE-ID if Hit is YES, otherwise N/A]\n"
        "</Ground Truth Defect GT-ISSUE-1>\n"
        "<Ground Truth Defect GT-ISSUE-2>\n"
        "Defect Type: [Copy from the corresponding Ground Truth Defect input]\n"
        "Description: [Copy from the corresponding Ground Truth Defect input]\n"
        "Hit: [YES or NO]\n"
        "Hit by: [PRED-ISSUE-ID if Hit is YES, otherwise N/A]\n"
        "</Ground Truth Defect GT-ISSUE-2>\n"
        "[... Continue for all Ground Truth Defects listed in the input ...]\n"
        "**END OF EVALUATION RESULT**\n"
    )


def create_messages(message, system_message=None):
    messages = [{"role": "system", "content": system_message}] if system_message is not None else []
    messages.append({"role": "user", "content": message})
    return messages


def parse_defect_pr_answer(args, instance, answer, logger):
    prompt = (
        "You are a data extraction assistant. Your task is to meticulously parse the provided text, which contains an evaluation result, and transform it into a structured JSON object.\n\n"
        "**Input Text Format:**\n"
        "The input text starts with an assessment of whether the PR was incorrectly identified as clean/good, followed by blocks describing predicted defects and ground truth defects, following this pattern:\n"
        "**1. Clean/Good Identification Assessment:**\n"
        "Incorrectly Identified as Good: [YES or NO]\n\n"
        "**2. Analysis of Predicted Review Defects:**\n"
        "[Optional: Text indicating no issues were raised OR <Predicted Defect ...> blocks]\n"
        "<Predicted Defect PRED-ISSUE-ID>\n"
        "Description: [Text describing the predicted defect]\n"
        "Severity Score: [A score from 0-10]\n"
        "</Predicted Defect PRED-ISSUE-ID>\n\n"
        "**3. Ground Truth Defect Coverage:**\n"
        "<Ground Truth Defect GT-ISSUE-ID>\n"
        "Defect Type: [Text describing the defect type]\n"
        "Description: [Text describing the ground truth defect]\n"
        "Hit: [YES or NO]\n"
        "Hit by: [PRED-ISSUE-ID if Hit is YES, otherwise N/A or potentially blank]\n"
        "</Ground Truth Defect GT-ISSUE-ID>\n\n"
        "There can be multiple <Predicted Defect ...> blocks and multiple <Ground Truth Defect ...> blocks.\n\n"
        "**Parsing Instructions:**\n"
        "1.  Extract the value ('YES' or 'NO') following 'Incorrectly Identified as Clean/Good: '.\n"
        "2.  Check section 2. Identify *all* blocks starting with `<Predicted Defect ...>`. If none are found, the `\"pred_issues\"` dictionary should be empty.\n"
        "3.  For each predicted defect block found in section 2:\n"
        "    - Extract the ID (e.g., 'PRED-ISSUE-1') from the tag.\n"
        "    - Extract the text following 'Description: '.\n"
        "    - Extract the number following 'Severity Score: ' and convert it to an **integer**.\n"
        "4.  Identify *all* blocks starting with `<Ground Truth Defect ...>` in section 3.\n"
        "5.  For each ground truth defect block found in section 3:\n"
        "    - Extract the ID (e.g., 'GT-ISSUE-1') from the tag.\n"
        "    - Extract the text following 'Defect Type: '.\n"
        "    - Extract the text following 'Description: '.\n"
        "    - Extract the value following 'Hit: ' (must be 'YES' or 'NO').\n"
        "    - Extract the value following 'Hit by: '. If 'Hit:' is 'NO', this value **must** be 'N/A' in the final JSON, even if the source text is blank or slightly different. If 'Hit:' is 'YES', this should be the corresponding PRED-ISSUE-ID.\n"
        "6.  Construct a single JSON object based on the extracted information.\n\n"
        "**Target JSON Structure:**\n"
        "The final output must be a JSON object with exactly three top-level keys:\n"
        "- `\"incorrectly_identified_as_good\"`: The extracted status string ('YES' or 'NO').\n"
        "- `\"pred_issues\"`: A dictionary where:\n"
        "    - Keys are the extracted predicted defect IDs (e.g., \"PRED-ISSUE-1\").\n"
        "    - Values are dictionaries containing:\n"
        "        - `\"description\"`: The extracted description string.\n"
        "        - `\"severity_score\"`: The extracted severity score as an **integer**.\n"
        "- `\"gt_issues\"`: A dictionary where:\n"
        "    - Keys are the extracted ground truth defect IDs (e.g., \"GT-ISSUE-1\").\n"
        "    - Values are dictionaries containing:\n"
        "        - `\"defect_type\"`: The extracted defect type string.\n"
        "        - `\"description\"`: The extracted description string.\n"
        "        - `\"hit\"`: The extracted hit status string ('YES' or 'NO').\n"
        "        - `\"hit_by\"`: The corresponding PRED-ISSUE-ID string if hit is 'YES', otherwise the string 'N/A'.\n\n"
        "**Example JSON Output:**\n"
        "```json\n"
        '{\n'
        '  "incorrectly_identified_as_good": "NO", \n'
        '  "pred_issues": {\n'
        '    "PRED-ISSUE-1": {"description": "Missing error handling for file open.", "severity_score": 8},\n'
        '    "PRED-ISSUE-2": {"description": "Potential off-by-one in loop.", "severity_score": 5}\n'
        '  },\n'
        '  "gt_issues": {\n'
        '    "GT-ISSUE-1": {"defect_type": "Error Handling", "description": "The function lacks error checking when opening config.yaml.", "hit": "YES", "hit_by": "PRED-ISSUE-1"},\n'
        '    "GT-ISSUE-2": {"defect_type": "Logic Error", "description": "The loop boundary should be less than count, not less than or equal.", "hit": "YES", "hit_by": "PRED-ISSUE-2"},\n'
        '    "GT-ISSUE-3": {"defect_type": "Security", "description": "User input is not sanitized before database query.", "hit": "NO", "hit_by": "N/A"}\n'
        '  }\n'
        '}\n'
        "```\n\n"
        "**Output Requirements:**\n"
        "- Return **ONLY** the JSON object.\n"
        "- Enclose the JSON object in a markdown code block starting with ```json.\n"
        "- Do **not** include any explanations, apologies, or introductory/concluding text outside the JSON code block.\n\n"
        f"**Text to Parse:**\n\n"
        f"```markdown\n{answer}\n```"
    )

    parsed_answer = run_chat(
        model=args.model, 
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0, 
        max_tokens=8192
    )
    if parsed_answer is None:
        logger.error("Failed to get parsing response from model for defect task.") # Specific log
        return None
    
    # Extract JSON using regex - use a more robust approach to find content between ```json and ``` markers
    json_str = None
    if '```json' in parsed_answer:
        start_marker = '```json'
        end_marker = '```'
        start_pos = parsed_answer.find(start_marker) + len(start_marker)
        
        # Find the last occurence of ``` that comes after the ```json marker
        remaining_text = parsed_answer[start_pos:]
        end_relative_pos = remaining_text.rfind(end_marker)
        if end_relative_pos != -1:
            end_pos = start_pos + end_relative_pos
            json_str = parsed_answer[start_pos:end_pos].strip()
    
    if not json_str:
        # Handle cases where the model might just return {} directly without markdown
        # Need to adjust this for the new structure
        stripped_answer = parsed_answer.strip()
        if stripped_answer.startswith('{') and stripped_answer.endswith('}'):
            try:
                potential_json = json.loads(stripped_answer)
                # Check for all three expected top-level keys
                if "incorrectly_identified_as_good" in potential_json and "pred_issues" in potential_json and "gt_issues" in potential_json:
                    json_str = stripped_answer # Assume it's valid JSON
            except json.JSONDecodeError:
                pass # Ignore if it's not valid JSON

        if not json_str:
            logger.error(f"No JSON block found or direct JSON parse failed in defect parsing response: {parsed_answer}")
            return None
        
    try:
        json_str = re.sub(r'[\x00-\x1F\x7F]', '', json_str)
        result = json.loads(json_str)
            
        # --- Start Validation for Defect Task Output --- 
        if "incorrectly_identified_as_good" not in result:
            logger.error(f"Missing 'incorrectly_identified_as_good' field: {json_str}")
            return None
            
        if result["incorrectly_identified_as_good"] not in ["YES", "NO"]:
            logger.error(f"Invalid value for 'incorrectly_identified_as_good': '{result['incorrectly_identified_as_good']}'. Must be 'YES' or 'NO'. JSON: {json_str}")
            return None

        # Validate required fields (pred_issues and gt_issues)
        if "pred_issues" not in result:
            logger.error(f"Missing 'pred_issues' field: {json_str}")
            return None
            
        if "gt_issues" not in result:
            logger.error(f"Missing 'gt_issues' field: {json_str}")
            return None

        if not isinstance(result['pred_issues'], dict):
             logger.error(f"'pred_issues' field is not a dictionary: {json_str}")
             return None
        if not isinstance(result['gt_issues'], dict):
             logger.error(f"'gt_issues' field is not a dictionary: {json_str}")
             return None
        
        # Validate predicted defects structure
        for defect_id, defect_info in result['pred_issues'].items():
            if not defect_id.startswith("PRED-ISSUE-"):
                 logger.warning(f"Unexpected format for predicted defect ID '{defect_id}'. Expected 'PRED-ISSUE-X'.")
            if "description" not in defect_info:
                logger.error(f"Missing 'description' field for defect {defect_id}: {json_str}")
                return None
            if "severity_score" not in defect_info:
                logger.error(f"Missing 'severity_score' field for defect {defect_id}: {json_str}")
                return None
            # Ensure severity scores are integers
            try:
                defect_info['severity_score'] = int(defect_info['severity_score'])
            except (ValueError, TypeError):
                 logger.error(f"'severity_score' is not an integer for defect {defect_id}: {json_str}")
                 return None
            
        # Validate ground truth defects structure
        gt_ids_from_instance = {f"GT-ISSUE-{i+1}" for i in range(len(instance.get('defects', [])))} # Handle case where instance might not have defects?
        for gt_id, gt_info in result['gt_issues'].items():
            if gt_id not in gt_ids_from_instance:
                 logger.error(f"Invalid or unexpected ground truth defect ID '{gt_id}': {json_str}")
                 return None
            if "defect_type" not in gt_info:
                logger.error(f"Missing 'defect_type' field for {gt_id}: {json_str}")
                return None
            if "description" not in gt_info:
                logger.error(f"Missing 'description' field for {gt_id}: {json_str}")
                return None
            if "hit" not in gt_info or gt_info['hit'] not in ["YES", "NO"]:
                logger.error(f"Missing or invalid 'hit' field ('{gt_info.get('hit')}') for {gt_id}: {json_str}")
                return None
            if "hit_by" not in gt_info:
                logger.error(f"Missing 'hit_by' field for {gt_id}: {json_str}")
                return None
            if gt_info['hit'] == "NO" and gt_info['hit_by'] != "N/A":
                 logger.warning(f"'hit_by' field is '{gt_info['hit_by']}' but 'hit' is NO for {gt_id}. Setting hit_by to N/A.")
                 gt_info['hit_by'] = "N/A"
            elif gt_info['hit'] == "YES" and not gt_info['hit_by'].startswith("PRED-ISSUE-"):
                 logger.error(f"Invalid 'hit_by' value ('{gt_info['hit_by']}') when hit is YES for {gt_id}: {json_str}")
                 return None
        # --- End Validation for Defect Task Output ---
        
        # Return the structure directly as parsed (no transformation needed anymore)
        return result
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON decode error during defect answer parsing: {e} in: {json_str}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing defect task answer: {e}")
        return None


def evaluate_one(args, item, logger):
    # system_message = "You are a helpful assistant."
    system_message = None
    defect_introduced = item['instance']['defect_introduced']
    if defect_introduced:
        prompt = create_defect_pr_prompt(item)
    else:
        prompt = create_clean_pr_prompt(item)
    instance_id = item['instance']['instance_id']
    messages = create_messages(
        message=prompt,
        system_message=system_message
    )
    logger.debug(f"Sending request for instance {instance_id} ...")
    logger.debug(f"Prompt: \n{prompt}")
    result = None # Initialize result
    response = None # Initialize response
    for i in range(3):
        response = run_chat(
            model=args.model,
            messages=messages,
            temperature=args.temperature,
            max_tokens=args.max_tokens
        )
        if response is None:
            logger.warning(f"Failed to get response for instance {instance_id} (Attempt {i+1})")
            continue # Try again
        logger.debug(f"Received response for instance {instance_id}: {response}")

        # Determine which parser to use based on whether defects were introduced
        if defect_introduced:
            result = parse_defect_pr_answer(args, item['instance'], response, logger)
        else:
            result = parse_clean_pr_answer(args, item['instance'], response, logger)

        if result is not None:
            logger.debug(f"Successfully parsed answer for instance {instance_id} (Attempt {i+1})")
            break # Exit loop if parsing is successful
        else:
            logger.warning(f"Failed to parse answer for instance {instance_id} (Attempt {i+1}). Retrying...")
            # Optionally add the failed response to messages if retrying with context is desired
            # messages.append({"role": "assistant", "content": response})
            # messages.append({"role": "user", "content": "Parsing failed. Please provide the output strictly in the specified JSON format."})


    if result is None:
        logger.error(f"Failed to get and parse answer for instance {instance_id} after 3 attempts.")
        # Store the last failed response if needed for debugging
        # return {"instance_id": instance_id, "error": "Parsing failed", "last_response": response}
        return None # Return None if all attempts fail

    # Append the successful assistant response
    messages.append({"role": "assistant", "content": response})

    # Process result based on task type
    if defect_introduced:
        # Add defect types back to gt_issues
        for gt_defect_id, gt_info in result.get('gt_issues', {}).items():
             # Assuming format is always GT-ISSUE-<number>
            try:
                defect_index_str = gt_defect_id.split('-ISSUE-')[-1]
                defect_id = int(defect_index_str) - 1
                if 0 <= defect_id < len(item['instance']['defects']):
                    result['gt_issues'][gt_defect_id]['defect_type'] = item['instance']['defects'][defect_id]['defect_type']
                    # Ensure the description from the instance is used for consistency?
                    # result['gt_issues'][gt_defect_id]['description'] = item['instance']['defects'][defect_id]['defect_discussion']['description']
                else:
                    logger.error(f"Parsed defect index {defect_id} out of bounds for instance {instance_id}. GT ID: '{gt_defect_id}'")
            except (IndexError, ValueError) as e:
                logger.error(f"Could not parse defect index from GT ID '{gt_defect_id}': {e}")

        # Calculate hit rate
        hit_count = sum(1 for gt_info in result['gt_issues'].values() if gt_info['hit'].lower() == 'yes')
        total_count = len(result['gt_issues'])

        return {
            "instance_id": instance_id,
            "defect_introduced": True,
            "hit": hit_count,
            "total": total_count,
            "pred_issues": result['pred_issues'],
            "gt_issues": result['gt_issues'],
            "incorrectly_identified_as_good": result['incorrectly_identified_as_good'],
            "response": response, # Store the successful response
            "review": item['pred']['review'],
            "messages": messages # Store the conversation history
        }
    else: # Clean PR case
         return {
             "instance_id": instance_id,
             "defect_introduced": False,
             "correctly_identified_as_good": result['correctly_identified_as_good'],
             "pred_issues": result['pred_issues'],
             "response": response, # Store the successful response
             "review": item['pred']['review'],
             "messages": messages # Store the conversation history
         }


def analyze_result(results):
    # Helper function for P/R/F1 calculation
    def calculate_prf1(tp, fp, fn):
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        # Ensure results are serializable if they end up in JSON
        precision = float(precision) if not isinstance(precision, (int, float)) else precision
        recall = float(recall) if not isinstance(recall, (int, float)) else recall
        f1 = float(f1) if not isinstance(f1, (int, float)) else f1
        return round(precision, 4), round(recall, 4), round(f1, 4)
        
    def analyze_one(input_results, is_defect_task):
        if not input_results: # Handle empty list
             if is_defect_task:
                 # Return structure for defect task: tp, fp, fn, total_gt, severity_list, num_pred_issues_list, identified_good_list
                 return 0, 0, 0, 0, [], [], []
             else:
                 # Return structure for clean task: severe_fp_list, severity_scores, num_pred_issues_list, identified_good_list
                 return [], [], [], []

        if is_defect_task:
            total_hits = 0
            total_gt_defects = 0
            total_fp = 0
            severity_list = [] 
            num_pred_issues_list = []
            identified_good_list = [] 

            for res in input_results:
                # Aggregate TP, Total GT
                hits = res.get('hit', 0)
                total = res.get('total', 0)
                total_hits += hits
                total_gt_defects += total
                
                # Aggregate FP (predicted issues in this instance that didn't hit any GT issue)
                pred_issues = res.get('pred_issues', {})
                gt_issues = res.get('gt_issues', {})
                pred_issue_ids = set(pred_issues.keys())
                hit_by_ids = set()
                for gt_info in gt_issues.values():
                    hit_by = gt_info.get('hit_by')
                    if hit_by and hit_by != 'N/A' and hit_by in pred_issue_ids:
                        hit_by_ids.add(hit_by)
                fp_instance = len(pred_issue_ids - hit_by_ids)
                total_fp += fp_instance

                # Collect severity and count info
                instance_severities = []
                if pred_issues: 
                    instance_severities = [d.get('severity_score', 0) for d in pred_issues.values()] # Use .get
                    # Check if severity_list needs individual scores or instance averages
                    # Adding instance averages based on previous logic
                    avg_severity_instance = np.mean(instance_severities) 
                    severity_list.append(avg_severity_instance) 
                    num_pred_issues_list.append(len(pred_issues))
                else:
                    num_pred_issues_list.append(0)
                
                # Collect identified as good info (all pred severities <= 3 or no issues)
                all_low_severity = all(s <= 3 for s in instance_severities)
                identified_good_list.append(1 if all_low_severity else 0)

            # Calculate final TP, FP, FN for the given input_results subset
            tp = total_hits
            fn = total_gt_defects - total_hits
            fp = total_fp # Already aggregated

            # Return aggregated values
            # Note: severity_list contains instance averages, not individual scores based on previous logic
            return tp, fp, fn, total_gt_defects, severity_list, num_pred_issues_list, identified_good_list
        
        else: # Clean task analysis
            severe_fp_list = [] 
            severity_scores = [] # List of individual severity scores for all FPs
            num_pred_issues_list = []
            identified_good_list = []

            for res in input_results:
                instance_severities = []
                has_severe_fp = False
                pred_issues = res.get('pred_issues', {})
                if pred_issues:
                    instance_severities = [iss.get('severity_score', 0) for iss in pred_issues.values()] # Use .get
                    severity_scores.extend(instance_severities) # Collect all individual FP severities
                    num_pred_issues_list.append(len(pred_issues))
                    # Check if any FP has severity > 3
                    if any(s > 3 for s in instance_severities):
                        has_severe_fp = True
                else:
                     num_pred_issues_list.append(0)
                
                severe_fp_list.append(1 if has_severe_fp else 0)

                # Check if identified as good (all severities <= 3 or no issues)
                all_low_severity = all(s <= 3 for s in instance_severities)
                identified_good_list.append(1 if all_low_severity else 0)
                
            # Return aggregated lists for clean tasks
            return severe_fp_list, severity_scores, num_pred_issues_list, identified_good_list

    valid_results = [result for result in results if result is not None] # Simplified check

    analysis_results = {'submit': len(results), 'valid': len(valid_results), 'error': len(results) - len(valid_results)}

    # --- Overall PR Classification (Clean vs. Defective) ---
    tp_all, fp_all, fn_all, tn_all = 0, 0, 0, 0
    for res in valid_results:
        is_defect = res.get('defect_introduced')
        if is_defect is None: 
            continue # Skip if defect status is unknown

        if is_defect: # Actually Defective PR
            # Predicted Defective if NOT incorrectly identified as good
            # Important: The flag 'incorrectly_identified_as_good' means the model SAID it was good (incorrectly)
            # So, Predicted Defective = (incorrectly_identified_as_good == 'NO')
            predicted_clean = res.get('incorrectly_identified_as_good') == 'YES'
            if predicted_clean:
                fn_all += 1 # It was defective, but model predicted clean (False Negative)
            else:
                tp_all += 1 # It was defective, and model predicted defective (True Positive)
        else: # Actually Clean PR
            # Predicted Clean if correctly identified as good
            predicted_clean = res.get('correctly_identified_as_good') == 'YES'
            if predicted_clean:
                tn_all += 1 # It was clean, and model predicted clean (True Negative)
            else: 
                fp_all += 1 # It was clean, but model predicted defective (False Positive)

    precision_all, recall_all, f1_all = calculate_prf1(tp_all, fp_all, fn_all)
    # Calculate overall accuracy
    total_all = tp_all + tn_all + fp_all + fn_all
    accuracy_all = (tp_all + tn_all) / total_all if total_all > 0 else 0
    accuracy_all = float(accuracy_all) if not isinstance(accuracy_all, (int, float)) else accuracy_all
    accuracy_all = round(accuracy_all, 4)

    analysis_results['overall_classification'] = {
        "accuracy": accuracy_all, # Added accuracy
        "precision": precision_all,
        "recall": recall_all,
        "f1": f1_all,
        "tp": tp_all,
        "fp": fp_all,
        "fn": fn_all,
        "tn": tn_all,
        "count": len(valid_results)
    }
    
    # Separate results for defect and clean tasks
    defect_results = [res for res in valid_results if res.get('defect_introduced') is True] # Explicit check for True
    clean_results = [res for res in valid_results if res.get('defect_introduced') is False] # Explicit check for False

    # Analyze All Defect Results
    if defect_results:
         # Unpack results from analyze_one for defect PRs
         # Signature: tp, fp, fn, total_gt, severity_list, num_pred_issues_list, identified_good_list
         tp_defect_task, fp_defect_task, fn_defect_task, total_gt_defects, severity_list, num_pred_issues_list, identified_good_list = analyze_one(defect_results, is_defect_task=True) 

         # Calculate overall metrics from aggregated values
         hit_rate = round(tp_defect_task / total_gt_defects, 4) if total_gt_defects > 0 else 0
         avg_pred_issue_severity = np.mean(severity_list).round(4) if severity_list else 0
         avg_pred_issues = np.mean(num_pred_issues_list).round(4) if num_pred_issues_list else 0
         identified_as_good_rate = np.mean(identified_good_list).round(4) if identified_good_list else 0
         precision_defect, recall_defect, f1_defect = calculate_prf1(tp_defect_task, fp_defect_task, fn_defect_task)

         analysis_results['defects_all'] = {
             "hit_rate": hit_rate,
             "avg_pred_issue_severity": avg_pred_issue_severity, 
             "avg_pred_issues": avg_pred_issues, # Changed key name
             "identified_as_good_rate": identified_as_good_rate, # NEW metric, replacing incorrectly_identified_as_good_rate
             "precision": precision_defect,
             "recall": recall_defect,
             "f1": f1_defect,
             "tp": tp_defect_task,
             "fp": fp_defect_task,
             "fn": fn_defect_task,
             "count": len(defect_results)
         }
         
    # Analyze All Clean Results
    if clean_results:
        # Unpack results from analyze_one for clean PRs
        # Signature: severe_fp_list, severity_scores, num_pred_issues_list, identified_good_list
        severe_fp_list, severity_scores, num_pred_issues_list, identified_good_list = analyze_one(clean_results, is_defect_task=False) 

        # Calculate overall metrics from aggregated values
        severe_false_positive_rate = np.mean(severe_fp_list).round(4) if severe_fp_list else 0
        avg_pred_issue_severity = np.mean(severity_scores).round(4) if severity_scores else 0
        avg_issues = np.mean(num_pred_issues_list).round(4) if num_pred_issues_list else 0
        identified_as_good_rate = np.mean(identified_good_list).round(4) if identified_good_list else 0

        analysis_results['clean_all'] = {
            "severe_false_positive_rate": severe_false_positive_rate, # NEW metric, replacing false_positive_rate
            "avg_pred_issue_severity": avg_pred_issue_severity, 
            "avg_pred_issues": avg_issues,
            "identified_as_good_rate": identified_as_good_rate, # NEW metric, replacing correctly_identified_as_good_rate
            "count": len(clean_results)
        }
        
    if defect_results:
         # Group defect results by defect types in gt_issues
         results_by_defect_type = {}
         for result in defect_results:
             # Use gt_issues if available
             if 'gt_issues' in result:
                  for gt_id, gt_defect in result['gt_issues'].items(): # Iterate through items
                      defect_type = gt_defect.get('defect_type', 'Unknown') # Handle missing type
                      if defect_type not in results_by_defect_type:
                          results_by_defect_type[defect_type] = []
                      # Add instance to list for this type if not already added
                      # We want to analyze all instances that *contain* this defect type
                      # Check instance_id to prevent duplicates if an instance has multiple defects of the same type
                      if result['instance_id'] not in [r['instance_id'] for r in results_by_defect_type[defect_type]]:
                           results_by_defect_type[defect_type].append(result)

         # Analyze defect results by type
         analysis_results['defects_by_type'] = {}
         for defect_type, type_results in results_by_defect_type.items():
             if type_results: # Ensure list is not empty
                 # --- Calculate P/R/F1 specific to this defect type ---
                 tp_type, fp_type, fn_type = 0, 0, 0
                 total_gt_type = 0

                 for res in type_results:
                     gt_issues = res.get('gt_issues', {})
                     pred_issues = res.get('pred_issues', {})

                     # Calculate TP and total GT for this type in this instance
                     gt_issues_of_type = {gt_id: gt_info for gt_id, gt_info in gt_issues.items() if gt_info.get('defect_type') == defect_type}
                     hits_instance_type = sum(1 for gt_info in gt_issues_of_type.values() if gt_info['hit'].lower() == 'yes')
                     total_instance_type = len(gt_issues_of_type)

                     tp_type += hits_instance_type
                     total_gt_type += total_instance_type

                     # Calculate FP for this instance (predicted issues that hit nothing)
                     pred_issue_ids = set(pred_issues.keys())
                     hit_by_ids = set()
                     for gt_info in gt_issues.values(): # Check against ALL gt_issues in the instance
                         hit_by = gt_info.get('hit_by')
                         if hit_by and hit_by != 'N/A' and hit_by in pred_issue_ids:
                             hit_by_ids.add(hit_by)
                     fp_instance = len(pred_issue_ids - hit_by_ids)
                     fp_type += fp_instance # Add instance FPs to the type's FP count

                 # Calculate FN for the type
                 fn_type = total_gt_type - tp_type

                 # Calculate type-specific P/R/F1
                 precision_type, recall_type, f1_type = calculate_prf1(tp_type, fp_type, fn_type)

                 # Filter results specific to this defect type for hit rate calculation
                 # Note: recall_type calculated above is equivalent to hit_rate_for_type
                 specific_type_hits = 0
                 specific_type_total = 0
                 for res in type_results:
                      gt_issues_of_type = {gt_id: gt_info for gt_id, gt_info in res['gt_issues'].items() if gt_info.get('defect_type') == defect_type}
                      specific_type_hits += sum(1 for gt_info in gt_issues_of_type.values() if gt_info['hit'].lower() == 'yes')
                      specific_type_total += len(gt_issues_of_type)

                 hit_rate_for_type = round(specific_type_hits / specific_type_total, 4) if specific_type_total > 0 else 0

                 # Calculate average inherent severity and avg_pred_issues over all results containing this type
                 # Call analyze_one on the subset to get average severity and issues for instances with this type
                 _, _, _, _, severity_list_type, num_pred_issues_list_type, _ = analyze_one(type_results, is_defect_task=True)
                 avg_pred_issue_severity_for_type = np.mean(severity_list_type).round(4) if severity_list_type else 0
                 avg_pred_issues_for_type = np.mean(num_pred_issues_list_type).round(4) if num_pred_issues_list_type else 0

                 analysis_results['defects_by_type'][defect_type] = {
                     "precision": precision_type,
                     "recall": recall_type, # Same as hit_rate_for_type
                     "f1": f1_type,
                     "tp": tp_type,
                     "fp": fp_type,
                     "fn": fn_type,
                     "avg_pred_issue_severity": avg_pred_issue_severity_for_type, 
                     "avg_pred_issues": avg_pred_issues_for_type, # Changed key name
                      # Note: identified_as_good_rate is not typically calculated per defect type, so omitting it here
                     "count": len(type_results) # Count of instances containing this defect type
                 }



    return analysis_results


def evaluate(args):
    random.seed(42)
    dataset = load_jsonl(args.dataset_file)
    dataset_dict = {item['instance_id']: item for item in dataset}
    predictions = load_jsonl(args.pred_file)
    random.shuffle(predictions)
    # predictions = predictions[:100]
    
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    
    log_file = args.output_file + ".log"
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[logging.FileHandler(log_file), logging.StreamHandler()]
    )
    logger = logging.getLogger(__name__)
    
    logger.info("="*100)
    logger.info(f"args: {args}")
    logger.info("="*100)
    logger.info(f"Evaluating {len(predictions)} instances")
    logger.info(f"Dataset File: {args.dataset_file}")
    logger.info(f"Model: {args.model}")
    logger.info(f"Output File: {args.output_file}")
    logger.info("="*100)
    
    def process_item(item):
        result = evaluate_one(args, item, logger)
        return result

    results = []
    tasks = [{'pred': pred, 'instance': dataset_dict[pred['instance_id']]} for pred in predictions]
    with ThreadPoolExecutor(max_workers=args.num_threads) as executor:
        futures = [executor.submit(process_item, item) for item in tasks]
        for future in tqdm(as_completed(futures), total=len(tasks), desc="Processing"):
            result = future.result()
            results.append(result)
    
    # results = []
    # for item in tqdm(tasks, total=len(tasks), desc="Processing"):
    #     result = process_item(item)
    #     results.append(result)
    
    if len(results) > 0:
        save_json(results, args.output_file + ".tmp.json")
        
    if len(results) == 0 and os.path.exists(args.output_file + ".tmp.json"):
        results = load_json(args.output_file + ".tmp.json")
    
    analysis_results = analyze_result(results)
    logger.info(f"Analysis Results: \n{json.dumps(analysis_results, indent=4)}")
    
    save_result = {
        "analysis_results": analysis_results,
        "details": results
    }
    
    save_json(save_result, args.output_file)
    logger.info(f"Finished processing {len(dataset)} instances")
    logger.info(f"Output File: {args.output_file}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset-file", type=str, help="Path to dataset file")
    parser.add_argument("--pred-file", type=str, help="Path to predictions file")
    parser.add_argument("--output-file", type=str, help="Path to output file")
    parser.add_argument("--model", type=str, help="Model name")
    parser.add_argument("--instance-ids", nargs="+", help="Instance ids")
    parser.add_argument("--ignore-ids", nargs="+", help="Ignore instance ids")
    parser.add_argument("--num-threads", default=1, type=int, help="Number of threads")
    parser.add_argument("--temperature", default=0.0, type=float, help="Temperature")
    parser.add_argument("--max-tokens", default=8192, type=int, help="Max tokens")
    args = parser.parse_args()

    # evaluate
    evaluate(args)
    
# TODO: Add actual modified location and code snippet